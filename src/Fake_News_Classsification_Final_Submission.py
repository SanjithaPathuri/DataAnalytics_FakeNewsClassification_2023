# -*- coding: utf-8 -*-
"""Update1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ywc-8ITJ5vQy6EOxhuCE18usc-Xq9bWy

### *CS579 ONLINE SOCIAL NETWORK ANALYSIS* 
*FAKE NEWS CLASSIFICATION*

By:  
*   Mohan Babu Kunchala (A20524765)
*   Sanjitha Reddy Pathuri (A20524383)
"""

#Import the necessary libraries to build a classifier: pandas, numpy, sklearn, and nltk
import pandas as pd
import numpy as np
import scipy.sparse as sp
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import string
import nltk 
nltk.download('punkt')
nltk.download('stopwords')

# Load the training data
train_data = pd.read_csv('train.csv')

#Remove any duplicate rows, missing values, or irrelevant columns:
train_data.drop_duplicates(inplace=True) 
train_data.dropna(inplace=True) 
train_data.drop(columns=['id', 'tid1', 'tid2'], inplace=True)

# Defining a function to clean the text: Convert the text data into lowercase, remove any punctuations, and apply stemming
stopwords = set(nltk.corpus.stopwords.words('english')) 
def clean_text(text):
    text = text.lower()
    tokens = nltk.word_tokenize(text) 
    tokens = [token for token in tokens if token.isalpha() and token not in stopwords] 
    text = ' '.join(tokens) 
    text = ''.join(c for c in text if c.isalnum() or c.isspace())
    return text

# Preprocess the data by applying the clean_text function n each pair f texts
train_data['title1_en_clean'] = train_data['title1_en'].apply(clean_text)
train_data['title2_en_clean'] = train_data['title2_en'].apply(clean_text)

# Vectorize the text using TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()
X1 = tfidf_vectorizer.fit_transform(train_data['title1_en_clean'])
X2 = tfidf_vectorizer.transform(train_data['title2_en_clean'])
X = sp.hstack((X1, X2))
y = train_data['label']

# Split the preprocessed data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Train different classifiers and evaluate accuracy of each classifier on the validation set

### Logical Regression

# Train the classifier
lr_classifier = LogisticRegression(random_state=42, max_iter=1000)
lr_classifier.fit(X_train, y_train)

# Evaluate the model
y_pred_lr = lr_classifier.predict(X_val)
accuracy_lr = accuracy_score(y_val, y_pred_lr)
precision_lr = precision_score(y_val, y_pred_lr, average='weighted')
recall_lr = recall_score(y_val, y_pred_lr, average='weighted')
f1_score_lr = f1_score(y_val, y_pred_lr, average='weighted')
print('## Logical Regression ##')
print('Accuracy:', accuracy_lr)
print('Precision:', precision_lr)
print('Recall:', recall_lr)
print('F-1 Score:', f1_score_lr)
print('Confusion Matrix:')
print(confusion_matrix(y_val,y_pred_lr))
print('Classification Report:')
print(classification_report(y_val,y_pred_lr))

### Naive Bayes Classifier

# Train the classifier
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train, y_train)

# Evaluate the model
y_pred_nb = nb_classifier.predict(X_val)
accuracy_nb = accuracy_score(y_val, y_pred_nb)
precision_nb = precision_score(y_val, y_pred_nb, average='weighted')
recall_nb = recall_score(y_val, y_pred_nb, average='weighted')
f1_score_nb = f1_score(y_val, y_pred_nb, average='weighted')
print('## Naive Bayes classifier ##')
print('Accuracy:', accuracy_nb)
print('Precision:', precision_nb)
print('Recall:', recall_nb)
print('F-1 Score:', f1_score_nb)
print('Confusion Matrix:')
print(confusion_matrix(y_val,y_pred_nb))
print('Classification Report:')
print(classification_report(y_val,y_pred_nb))

### Random forest

# Train the classifier
rf_classifier = RandomForestClassifier(n_estimators=25, random_state=42)
rf_classifier.fit(X_train, y_train)

# Evaluate the model
y_pred_rf = rf_classifier.predict(X_val)
accuracy_rf = accuracy_score(y_val, y_pred_rf)
precision_rf = precision_score(y_val, y_pred_rf, average='weighted')
recall_rf = recall_score(y_val, y_pred_rf, average='weighted')
f1_score_rf = f1_score(y_val, y_pred_rf, average='weighted')
print('## Random Forest ##')
print('Accuracy:', accuracy_rf)
print('Precision:', precision_rf)
print('Recall:', recall_rf)
print('F-1 Score:', f1_score_rf)
print('Confusion Matrix:')
print(confusion_matrix(y_val,y_pred_rf))
print('Classification Report:')
print(classification_report(y_val,y_pred_rf))

### Linear Support Vector Classifier (SVC)

# Train the classifier
svc_classifier = LinearSVC(random_state=42)
svc_classifier.fit(X_train, y_train)

# Evaluate the model
y_pred_svc = svc_classifier.predict(X_val)
accuracy_svc = accuracy_score(y_val, y_pred_svc)
precision_svc = precision_score(y_val, y_pred_svc, average='weighted')
recall_svc = recall_score(y_val, y_pred_svc, average='weighted')
f1_score_svc = f1_score(y_val, y_pred_svc, average='weighted')
print('## Support Vector classifier ##')
print('Accuracy:', accuracy_svc)
print('Precision:', precision_svc)
print('Recall:', recall_svc)
print('F-1 Score:', f1_score_svc)
print('Confusion Matrix:')
print(confusion_matrix(y_val,y_pred_svc))
print('Classification Report:')
print(classification_report(y_val,y_pred_svc))

### Stochastic Gradient Descent (SGD) Classifier

# Train the classifier
sgd_classifier = SGDClassifier(random_state=42, max_iter=1000)
sgd_classifier.fit(X_train, y_train)

# Evaluate the model
y_pred_sgd = sgd_classifier.predict(X_val)
accuracy_sgd = accuracy_score(y_val, y_pred_sgd)
precision_sgd = precision_score(y_val, y_pred_sgd, average='weighted')
recall_sgd = recall_score(y_val, y_pred_sgd, average='weighted')
f1_score_sgd = f1_score(y_val, y_pred_sgd, average='weighted')
print('## SGD classifier ##')
print('Accuracy:', accuracy_sgd)
print('Precision:', precision_sgd)
print('Recall:', recall_sgd)
print('F-1 Score:', f1_score_sgd)
print('Confusion Matrix:')
print(confusion_matrix(y_val,y_pred_sgd))
print('Classification Report:')
print(classification_report(y_val,y_pred_sgd))

# Loads test data, preprocess it using the same steps as train data
test_data = pd.read_csv('test.csv')
test_data.drop(columns=['id', 'tid1', 'tid2'], inplace=True)
test_data['title1_en_clean'] = test_data['title1_en'].apply(clean_text)
test_data['title2_en_clean'] = test_data['title2_en'].apply(clean_text)

X1_test = tfidf_vectorizer.transform(test_data['title1_en_clean'])
X2_test = tfidf_vectorizer.transform(test_data['title2_en_clean'])
X_test = sp.hstack((X1_test, X2_test))

# Make predictions on the test data using each classifier
y_pred_test_lr = lr_classifier.predict(X_test)
y_pred_test_nb = nb_classifier.predict(X_test)
y_pred_test_rf = rf_classifier.predict(X_test)
y_pred_test_svc = svc_classifier.predict(X_test)
y_pred_test_sgd = sgd_classifier.predict(X_test)

# Create a submission file for each classifier by saving the predicted labels for test data in the same format as sample_submission file
submission = pd.read_csv('sample_submission.csv')

submission['label'] = y_pred_test_lr
submission.to_csv('submission_lf.csv', index=False)

submission['label'] = y_pred_test_nb
submission.to_csv('submission_nb.csv', index=False)

submission['label'] = y_pred_test_svc
submission.to_csv('submission_svc.csv', index=False)

submission['label'] = y_pred_test_sgd
submission.to_csv('submission_sgd.csv', index=False)

#Saving the Random forest classifier as the actual submission file as the accuracy is more than the other classifiers used
submission['label'] = y_pred_test_rf
submission.to_csv('submission.csv', index=False)